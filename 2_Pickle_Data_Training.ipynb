{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayeelim/AIROST_BrainTumorClassification/blob/main/2_Pickle_Data_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2dvYRTkxHfH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSwbb8j5xY4k",
        "outputId": "edf4b7f2-dea9-4ce3-bac6-ac47fffd5a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Training Data"
      ],
      "metadata": {
        "id": "2kxI1H7HFVJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HuxwAlTxcMM"
      },
      "outputs": [],
      "source": [
        "TEST_DIR = '/content/gdrive/My Drive/AIROST/Dataset/Testing' # test data folder\n",
        "TRAIN_DIR = '/content/gdrive/My Drive/AIROST/Dataset/Training' # train data folder\n",
        "IMG_SIZE = 150 # image size\n",
        "CATEGORIES = [\"Train-glioma-output\",\"Train-meningioma-output\",\"Train-no_tumor-output\",\"Train-pituitary-output\"] #\"Train-glioma-output\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZxUTfnlxqOG",
        "outputId": "107cdd8e-669b-4566-a0d4-57715c89f6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28918/28918 [01:22<00:00, 350.05it/s]\n",
            "100%|██████████| 17240/17240 [00:58<00:00, 294.46it/s]\n",
            "100%|██████████| 8285/8285 [00:27<00:00, 300.70it/s]\n",
            "100%|██████████| 17348/17348 [01:02<00:00, 275.73it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71791\n",
            "train\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(TRAIN_DIR,category)  # create path\n",
        "        class_num = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "          # iterate over each image\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "          training_data.append([new_array, class_num])  # add this to our training_data\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "    #np.save('train_data1.npy', training_data)\n",
        "\n",
        "create_training_data()\n",
        "np.save('/content/gdrive/My Drive/AIROST/Dataset/train_data.npy', training_data)\n",
        "print(len(training_data))\n",
        "\n",
        "print(\"train\")\n",
        "print()\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_train = [i[1] for i in training_data]\n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"/content/gdrive/My Drive/AIROST/Dataset/X_train.pickle\",\"wb\")\n",
        "pickle.dump(X_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"/content/gdrive/My Drive/AIROST/Dataset/Y_train.pickle\",\"wb\")\n",
        "pickle.dump(Y_train, pickle_out)\n",
        "pickle_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File too big. Try to use batch"
      ],
      "metadata": {
        "id": "pepnE3DpFb_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batch to save the file\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def data_generator(directory, categories, img_size, batch_size):\n",
        "    for category in categories:\n",
        "        path = os.path.join(directory, category)\n",
        "        class_num = categories.index(category)\n",
        "\n",
        "        batch_data = []\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "            batch_data.append((new_array, class_num))\n",
        "\n",
        "            if len(batch_data) == batch_size:\n",
        "                # Yield the current batch and reset\n",
        "                yield batch_data\n",
        "                batch_data = []\n",
        "\n",
        "        # Yield the remaining data in the last batch\n",
        "        if batch_data:\n",
        "            yield batch_data\n",
        "\n",
        "def save_batches_to_pickle(directory, categories, img_size, batch_size, pickle_path):\n",
        "    data_gen = data_generator(directory, categories, img_size, batch_size)\n",
        "\n",
        "    for idx, batch_data in enumerate(data_gen):\n",
        "        X_batch = np.array([data[0] for data in batch_data]).reshape(-1, img_size, img_size, 3)\n",
        "        pickle_out = open(f\"{pickle_path}_batch{idx}.pickle\", \"wb\")\n",
        "        pickle.dump(X_batch, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "\n",
        "# Assuming the values of TRAIN_DIR, IMG_SIZE, BATCH_SIZE, and PICKLE_PATH are already defined\n",
        "TRAIN_DIR = '/content/gdrive/My Drive/AIROST/Dataset/Training' # train data folder\n",
        "IMG_SIZE = 150 # image size\n",
        "BATCH_SIZE = 30000\n",
        "categories = [\"Train-glioma-output\", \"Train-meningioma-output\", \"Train-no_tumor-output\", \"Train-pituitary-output\"]\n",
        "\n",
        "save_batches_to_pickle(TRAIN_DIR, categories, IMG_SIZE, BATCH_SIZE, \"/content/gdrive/My Drive/AIROST/Dataset/X_train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx2DjWdXjCNF",
        "outputId": "586470be-e397-4588-ff30-ac4bdee9bb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28918/28918 [02:07<00:00, 227.04it/s]\n",
            "100%|██████████| 17240/17240 [08:13<00:00, 34.91it/s] \n",
            "100%|██████████| 8285/8285 [03:40<00:00, 37.57it/s] \n",
            "100%|██████████| 17348/17348 [08:53<00:00, 32.52it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine the file\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def combine_batches(pickle_path, num_batches):\n",
        "    combined_data = []\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_file = f\"{pickle_path}_batch{i}.pickle\"\n",
        "\n",
        "        with open(batch_file, 'rb') as file:\n",
        "            batch_data = pickle.load(file)\n",
        "            combined_data.extend(batch_data)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# Example usage\n",
        "pickle_path = \"/content/gdrive/My Drive/AIROST/Dataset/X_train\"\n",
        "num_batches = 4  # Change this to the actual number of batches\n",
        "\n",
        "all_data = combine_batches(pickle_path, num_batches)\n",
        "\n",
        "#IMG_SIZE = 150 # image size\n",
        "\n",
        "# Convert the list of tuples to a numpy array\n",
        "#X_train_combined = np.array([data[0] for data in all_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "#print(X_train_combined.shape)\n",
        "#print(X_train_combined.size)\n",
        "\n",
        "# Save the combined data to a single pickle file\n",
        "combined_pickle_path = \"/content/gdrive/My Drive/AIROST/Dataset/X_train_combined.pickle\"\n",
        "with open(combined_pickle_path, 'wb') as file:\n",
        "    pickle.dump(all_data, file)\n"
      ],
      "metadata": {
        "id": "Js2tGyqoCzgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retry Using Original Data only<br>\n",
        "Exclude Augmented Data\n"
      ],
      "metadata": {
        "id": "RLoEZo6vH1n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4bdl0gcxH63h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITZdPFYRJpHl",
        "outputId": "9e4ce658-aae1-4ef4-96a9-5b3a5622f57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_DIR = '/content/gdrive/My Drive/AIROST/Dataset/Testing' # test data folder\n",
        "TRAIN_DIR = '/content/gdrive/My Drive/AIROST/Dataset/Training' # train data folder\n",
        "IMG_SIZE = 150 # image size\n",
        "CATEGORIES = [\"glioma_tumor\",\"meningioma_tumor\",\"no_tumor\",\"pituitary_tumor\"] #\"Train-glioma-output\","
      ],
      "metadata": {
        "id": "CaFIyAOUJpKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(TRAIN_DIR,category)  # create path\n",
        "        class_num = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "          # iterate over each image\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "          training_data.append([new_array, class_num])  # add this to our training_data\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "    #np.save('train_data1.npy', training_data)\n",
        "\n",
        "create_training_data()\n",
        "np.save('/content/gdrive/My Drive/AIROST/Dataset/train_data_new.npy', training_data)\n",
        "print(len(training_data))\n",
        "\n",
        "print(\"train\")\n",
        "print()\n",
        "X_train = np.array([i[0] for i in training_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_train = [i[1] for i in training_data]\n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"/content/gdrive/My Drive/AIROST/Dataset/X_train_new.pickle\",\"wb\")\n",
        "pickle.dump(X_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"/content/gdrive/My Drive/AIROST/Dataset/Y_train_new.pickle\",\"wb\")\n",
        "pickle.dump(Y_train, pickle_out)\n",
        "pickle_out.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jg4O6RPJpM0",
        "outputId": "fbe1bda1-4970-4759-f8e7-7db9cde837f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 826/826 [00:22<00:00, 36.45it/s] \n",
            "100%|██████████| 822/822 [00:16<00:00, 48.54it/s] \n",
            "100%|██████████| 395/395 [00:07<00:00, 53.70it/s] \n",
            "100%|██████████| 827/827 [00:15<00:00, 51.73it/s] \n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2870\n",
            "train\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "np2WK6E4MmCv",
        "outputId": "3e84e031-4e03-4384-c9d6-c19406b14a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2870, 150, 150, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}